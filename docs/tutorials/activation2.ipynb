{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{currentmodule} optimap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from optimap.utils import jupyter_render_animation as render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{tip}\n",
    "Download this tutorial as a {download}`Jupyter notebook <converted/activation.ipynb>`, or a {download}`python script <converted/activation.py>` with code cells. We highly recommend using [Visual Studio Code](#vscode) to execute this tutorial.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Activation Maps\n",
    "\n",
    "This tutorial demonstrates how to compute and visualize activation maps from cardiac optical mapping data using ``optimap``. Activation maps display local activation times (LATs), which indicate when the tissue becomes electrically activated. These maps are crucial for understanding cardiac electrical activity and identifying abnormalities.\n",
    "\n",
    "Computing local activation times corresponds to determining when the optical signal in a given pixel passes a certain pre-defined threshold or intensity value. For instance, if the optical trace is normalized and fluctuates between [0, 1] then the tissue could be defined as being 'electrically activated' when the time-series rises above or below 0.5 (depending on the fluorescent indicator and polarity of the signal).\n",
    "\n",
    "Here, we will use an example data from {cite:t}`Rybashlykov2022` in which a planar action potential wave propagates across the ventricle of a mouse heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimap as om\n",
    "import monochrome as mc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = om.download_example_data(\"doi:10.5281/zenodo.5557829/mouse_41_120ms_control_iDS.mat\")\n",
    "video = om.load_video(filename)\n",
    "metadata = om.load_metadata(filename)\n",
    "print(f\"Loaded video with shape {video.shape} and metadata {metadata}\")\n",
    "frequency = metadata[\"frequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mouse_41_120ms_control_iDS.mat` file from the [Zenodo dataset](https://doi.org/10.5281/zenodo.5557829) shows a induced pacing beats in a mouse heart. The {func}`load_metadata` function loads the metadata from the MATLAB file, in this case the acquisition frame rate. We visualize the video using [Monochrome](https://github.com/sitic/Monochrome):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Show video\n",
    "mc.show(video, name=filename.name, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video('https://cardiacvision.ucsf.edu/sites/g/files/tkssra6821/f/optimap-mouse_41_120ms_control_iDS_monochrome.mp4', html_attributes='controls autoplay loop width=\"100%\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ``optimap``'s {func}`background_mask` function to create a mask of the heart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove background by masking\n",
    "mask = om.background_mask(video[0], show=False)\n",
    "mask = om.image.dilate_mask(mask, iterations=5, show=False)\n",
    "om.image.show_mask(mask, video[0], title=\"Background mask\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing activation maps is highly noise-sensitive, so we need to need to apply strong filtering to the data. Here we use a spatio-temporal Gaussian filter and a spatial mean filter to smooth the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_filtered = om.video.smooth_spatiotemporal(video, sigma_temporal=1, sigma_spatial=1)\n",
    "video_filtered = om.video.mean_filter(video_filtered, size_spatial=5)\n",
    "\n",
    "# Normalize the video using a pixelwise sliding window\n",
    "video_norm = om.video.normalize_pixelwise_slidingwindow(video_filtered, window_size=200)\n",
    "video_norm[:, mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the mouse heart was stained with the voltage-sensitive dye Di-4-ANEPPS, the tissue becomes darker when it depolarizes (negative signal / polarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_video_pair(video, video_norm, title1=\"original video\",\n",
    "                   title2=\"normalized video\", interval=10)\n",
    "\n",
    "# Or in Monochrome:\n",
    "#\n",
    "# mc.show(video, name=\"original video\")\n",
    "# mc.show(video_norm, name=\"normalized video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_video_pair(video, video_norm, title1=\"original video\", title2=\"normalized video\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the {func}`activation.find_activations` function to find pacing beats. Here we just average the signal over the whole heart to identify when the pacing beat occurs. The function will return a list of activation times in frames and plot detected activations with a red line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = om.activation.find_activations(1 - video_norm, fps=frequency)\n",
    "print(f\"Found {len(activations)} activation events at frames: {activations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a trace position close to the pacing site \n",
    "#\n",
    "# traces, coords = om.select_traces(video_norm[:500], size=10, ref_frame=video[0])\n",
    "# trace = om.extract_traces(video_norm, coords[0], size=10)\n",
    "# activations = om.activation.find_activations(1 - trace)\n",
    "\n",
    "# Here hardcoded position (141, 100) for demo purposes\n",
    "fig, axs = plt.subplot_mosaic('ABB', figsize=(10, 2))\n",
    "om.show_positions([(141, 100)], video[0], ax=axs['A'])\n",
    "trace = om.extract_traces(video_norm, (141, 100), size=10)\n",
    "activations = om.activation.find_activations(1 - trace, ax=axs['B'], fps=frequency)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Found {len(activations)} activation events at frames: {activations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the video frames as the wave propagates across the ventricles for the first pacing beat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axs = plt.subplots(1, 7, figsize=(10, 3))\n",
    "axs[0].imshow(video[0], cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "for i in range(1, 7):\n",
    "    t = i * 2\n",
    "    axs[i].imshow(video_norm[activations[0] + t], cmap='gray', vmin=0, vmax=1)\n",
    "    axs[i].set_axis_off()\n",
    "    time = t * (1000/frequency)  # convert to ms\n",
    "    axs[i].set_title(f\"{time:.1f} ms\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute an activation map by identifying the local activation times in each pixel that correspond to when the action potential wave front passes through that pixel.\n",
    "\n",
    "## Computing Activation Maps from Pixel-wise Normalized Optical Maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pixel-wise normalization was sufficient as opposed to a sliding-window pixel-wise normalization, see [Tutorial 2](signal_extraction.ipynb), because we isolated a short part of the video that is only 20 frames long. In other cases it might be necessary to use a sliding-window pixel-wise normalization or a frame-wise difference video (e.g. with motion), see below.\n",
    "\n",
    "Let's plot some of the optical traces (manually selected so that they show locations which become subsequently activated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positions selected with the GUI:\n",
    "# positions = om.select_positions(video[0])\n",
    "positions =  [(134, 101), (14, 93), (94, 99), (53, 97)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "om.trace.show_positions(positions, video[0], ax=axs[0])\n",
    "traces = om.extract_traces(video_norm,\n",
    "                           positions,\n",
    "                           size=10,\n",
    "                           fps=frequency,\n",
    "                           ax=axs[1])\n",
    "axs[1].axhline(y=0.5, color='r', linestyle='dashed', label='threshold')\n",
    "axs[1].text(0.03, 0.52, 'threshold', color='r')\n",
    "plt.xlim(0, 0.12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ``optimap``'s {func}`compute_activation_map` function to automatically compute a two-dimensional activation map which shows the local activation times in every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = om.extract_traces(video_norm, (149, 115), size=10)\n",
    "activations = om.activation.find_activations(1 - trace, show=False)\n",
    "\n",
    "idx = activations[2]\n",
    "activation_map = om.compute_activation_map(video_norm[idx-3:idx + 16], inverted=True, fps=frequency, vmax=15, interpolate=False, show_contours=True, min_duration=0)\n",
    "\n",
    "# mc.show(activation_map, cmap='turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined mask manually with the GUI:\n",
    "#\n",
    "# mask = om.interactive_mask(image=video[0], initial_mask=mask)\n",
    "# om.save_mask('mouse_41_120ms_control_iDS_mask.png', mask)\n",
    "\n",
    "# Loading the mask from the file for demo purposes\n",
    "mask_filename = om.download_example_data('mouse_41_120ms_control_iDS_mask.png')\n",
    "mask = om.load_mask(mask_filename)\n",
    "video_norm[:, mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, {func}`compute_activation_map` and {func}`find_activation` calculates the local activation time (LAT) for each pixel as the time when the signal crosses a threshold of 0.5:\n",
    "\n",
    "$$\\text{LAT} = \\text{argmin}_{t} \\left( \\text{signal}_{t+1} > 0.5 \\right)$$\n",
    "\n",
    "The plot (left panel) below demonstrates this behavior, the vertical lines are the detected local activation times.\n",
    "\n",
    "If `invert=True` is set for {func}`find_activation`, the LAT is calculated as the time when the signal crosses a threshold of 0.5 in the opposite direction.\n",
    "\n",
    "When `interpolation=True` is set, the LAT is calculated as the fractional time between the two frames that cross the threshold using linear interpolation (right panel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax=axs[0]\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "for ax, interpolate in zip(axs, [False, True]):\n",
    "    om.show_traces(traces, ax=ax, colors=colors, linestyle='solid', marker='.')\n",
    "    activations = om.activation.find_activations(1 - traces, interpolate=interpolate, show=False)\n",
    "    for i in range(len(activations)):\n",
    "        for activation in activations[i]:\n",
    "            if 12 <= activation <= 28:\n",
    "                ax.axvline(activation, linestyle='--', color=colors[i], alpha=0.6)\n",
    "                ax.text(activation, 0.99, f'{activation:.1f}', \n",
    "                        rotation=90, va='top', ha='right', color=colors[i], fontsize=10)\n",
    "    ax.axhline(y=0.5, color='r', linestyle='dashed', label='threshold')\n",
    "    ax.text(25, 0.51, 'threshold', color='r')\n",
    "    ax.set_xlim(12, 28)\n",
    "    ax.grid()\n",
    "    ax.set_title(f\"interpolate=True\" if interpolate else \"interpolate=False (default)\")\n",
    "fig.suptitle(\"Non-interpolated vs interpolated LATs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the activation map of the last example when using linearly interpolated LATs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = om.compute_activation_map(video_norm[idx-3:idx + 16], inverted=True, interpolate=True, show_contours=True, fps=frequency, vmax=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Contour Lines to Activation Maps\n",
    "\n",
    "Contour lines are a powerful visualization tool that can help highlight the wavefront propagation. They connect points with the same activation time, making it easier to visualize the speed and direction of propagation. Let's add contour lines to our activation map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [3, 6, 9, 12, 15]\n",
    "# video_norm2 = om.video.mean_filter(video_norm, size_spatial=5)\n",
    "activation_map = om.compute_activation_map(video_norm[idx - 3:idx + 15], inverted=True, fps=frequency, vmax=15, show_contours=True, contour_levels=levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also combine contour lines with a raw image to visualize the propagation path over the heart tissue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "om.show_image(video[idx], ax=ax)\n",
    "# Add a red boundary around the mask\n",
    "mask_boundary = ax.contour(~mask, levels=[0], colors='black', linewidths=1.5, alpha=0.8)\n",
    "# Show contours\n",
    "om.show_activation_map(\n",
    "    activation_map,\n",
    "    ax=ax,\n",
    "    show_map=False,\n",
    "    show_contours=True,\n",
    "    contour_levels=range(2, 20, 2),\n",
    "    contour_fontsize=10,\n",
    "    contour_fmt='%1.0f ms',\n",
    "    contour_args={'linewidths': 1.5, 'alpha': 0.8, 'cmap': 'turbo', 'colors': None})\n",
    "# contour = ax.contour(activation_map, levels=contour_levels, cmap='turbo', linewidths=1.5, alpha=0.8)\n",
    "# ax.clabel(contour, fontsize=12, fmt='%1.0f ms')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the argument `inverted=True` due to the negative polarity of the signal ($- \\Delta F / F$). If me had manually inverted the video beforehand or with calcium imaging data this would not be necessary. The range of local activation times can be displayed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.print_properties(activation_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the local activation times are given in milliseconds (based on argument `fps`) and they range between 0ms and 18.4ms. The function {func}`compute_activation_map` uses {func}`show_activation_map` to plot the activation map (which can be disabled with argument `show=False`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.show_activation_map(activation_map, cmap=\"jet\", title='Activation Map', show_colorbar=True, colorbar_title='Activation Time [ms]', vmax=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have plotted the activation map using the `jet` colormap, here are some other options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "om.show_activation_map(activation_map, cmap='jet', show_colorbar=True, title='cmap=jet', ax=axs[0], colorbar_title=None)\n",
    "om.show_activation_map(activation_map, cmap='magma', show_colorbar=True, title='cmap=magma', ax=axs[1], colorbar_title=None)\n",
    "om.show_activation_map(activation_map, cmap='twilight_shifted', show_colorbar=True, title='cmap=twilight_shifted', ax=axs[2], colorbar_title=None)\n",
    "plt.suptitle('Activation maps with different colormaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = om.compute_activation_map(video_norm[idx - 5:idx + 20], inverted=True, interpolate=True)\n",
    "om.show_activation_map(activation_map)\n",
    "\n",
    "# Plot activation map as 3D surface\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.title('Activation Map 3D Surface')\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Activation Time [ms]')\n",
    "ax.set_title('Activation Map 3D Surface')\n",
    "# Create a meshgrid for the x and y coordinates\n",
    "x = np.arange(activation_map.shape[1])\n",
    "y = np.arange(activation_map.shape[0])\n",
    "X, Y = np.meshgrid(x, y)\n",
    "# Flatten the activation map for plotting\n",
    "Z = activation_map\n",
    "# Create a surface plot\n",
    "ax.plot_surface(X, Y, Z, cmap='turbo', edgecolor='none', linewidth=0, shade=False)\n",
    "# Set the view angle\n",
    "# ax.view_init(elev=30, azim=210)\n",
    "ax.view_init(elev=40, azim=-20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "\n",
    "plotter = pv.Plotter(notebook=True)\n",
    "plotter.set_background('white')\n",
    "\n",
    "x = np.arange(activation_map.shape[1]).astype(np.float32)\n",
    "y = np.arange(activation_map.shape[0]).astype(np.float32)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = activation_map.astype(np.float32)\n",
    "grid = pv.StructuredGrid(X, Y, Z)\n",
    "grid['Local Activation Time'] = Z.ravel()\n",
    "# grid.plot(cmap='turbo')\n",
    "plotter.add_mesh(grid, cmap='turbo')\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Activation Maps from Frame-Wise Difference Optical Maps\n",
    "\n",
    "In [Tutorial 2](signal_extraction.ipynb), we introduced the frame-wise difference method to emphasize sudden temporal changes in a video. Sudden temporal changes are caused by upstrokes of the action potential or calcium transients and the frame-wise difference filter is therefore ideally suited to visualize wavefronts as they propagate across the tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_diff = om.video.temporal_difference(video_filtered, 5)\n",
    "video_diff[:, mask] = np.nan\n",
    "video_diff_norm = om.video.normalize_pixelwise(video_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frame-wise difference approach enhances action potential upstroke, see the following video with temporal difference in the middle and our previous pixel-wise normalized video on the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_videos([video, video_diff_norm, video_norm],\n",
    "               titles=[\"original\", \"frame-wise diff\", \"pixelwise normalized\"],\n",
    "               interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_videos([video, video_diff_norm, video_norm],\n",
    "               titles=[\"original\", \"frame-wise diff\", \"pixelwise normalized\"],\n",
    "               interval=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the wavefront as an overlay over the raw (motion-stabilized) video. We will need to further post-process the data as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_diff[video_diff > 0] = 0\n",
    "video_diff_norm = om.video.normalize_pixelwise(-video_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action potential upstroke overlaid onto the raw video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.video.show_video_overlay(video,\n",
    "                            overlay=video_diff_norm,\n",
    "                            vmin_overlay=-1,\n",
    "                            vmax_overlay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.video.show_video_overlay(video, video_diff_norm, vmin_overlay=-1, vmax_overlay=1, interval=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "test_name": "notebook1"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
