{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{currentmodule} optimap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from optimap.utils import jupyter_render_animation as render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{tip}\n",
    "Download this tutorial as a {download}`Jupyter notebook <converted/overview.ipynb>`, or a {download}`python script <converted/overview.py>` with code cells. We highly recommend using [Visual Studio Code](#vscode) to execute this tutorial. Alternatively, you could run the Python script in a terminal with ``python overview.py`` from the folder where the file is located.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 0: Overview of optimap\n",
    "\n",
    "Welcome to optimap! This tutorial will give you a hands-on introduction to the main features of optimap, a Python library designed for analyzing fluorescent high-dynamic range video data. While our examples focus on cardiac optical mapping, optimap is versatile and can be used for many types of fluorescent imaging applications, including calcium imaging in neurons, fluorescent imaging in cell cultures, and other dynamic biological systems.\n",
    "\n",
    "**What is optical mapping?** Optical mapping is a technique used to record biological activity using fluorescent indicators. In cardiac research, these dyes change their fluorescence in response to changes in voltage or calcium, allowing researchers to visualize electrical waves spreading across the heart. Similar principles apply to other fluorescent imaging techniques in various biological systems.\n",
    "\n",
    "**What can optimap do?** Optimap helps you:\n",
    "- Load and visualize fluorescent video data from various sources and formats\n",
    "- Extract and analyze signals from specific regions of interest\n",
    "- Correct for tissue motion to improve signal quality (essential for many living samples)\n",
    "- Process and enhance signals to better visualize dynamic wave phenomena\n",
    "- Analyze spatiotemporal patterns in fluorescence data\n",
    "\n",
    "In this tutorial, we'll use cardiac data as an example, but the techniques demonstrated can be applied to other types of fluorescent imaging data. Let's get started by importing the optimap library. We'll use the short name `om` to make our code more concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optimap and use the short name 'om'\n",
    "import optimap as om"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's no error message after running this cell, optimap has been imported successfully! If you encounter an error, please check that optimap is properly installed (see [Installation Guide](#installing)).\n",
    "\n",
    "We'll also import two other common libraries that we will use in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy helps us work with numerical data arrays\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib helps us create plots and visualizations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Video Data\n",
    "\n",
    "Optimap can read many different video file formats used in optical mapping research, see [](io.ipynb) for further details.\n",
    "\n",
    ":::{admonition} File formats supported by {func}`optimap.load_video`\n",
    ":class: tip, dropdown\n",
    "\n",
    "* .tif, .tiff (TIFF) image stacks\n",
    "* Folder containing sequence of TIFF or .png (PNG) images\n",
    "* .mp4, .avi, .mov, … (digital video files)\n",
    "* .mat (MATLAB)\n",
    "* .npy (numpy array)\n",
    "* .gsd, .gsh (SciMedia MiCAM 05)\n",
    "* .rsh, .rsm, .rsd (SciMedia MiCAM ULTIMA)\n",
    "* .dat (MultiRecorder)\n",
    ":::\n",
    "\n",
    "Let's download an example video file and load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example file (only needs to be done once)\n",
    "filename = om.download_example_data(\"VF_Rabbit_1.npy\")\n",
    "\n",
    "# Load the video file\n",
    "# (If using your own data, replace the filename with your file path)\n",
    "video = om.load_video(filename)\n",
    "\n",
    "# Print information about the video\n",
    "om.print_properties(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above tells us important information about our video:\n",
    "\n",
    "- **Shape**: The video has 1000 frames, with each frame being 390×300 pixels\n",
    "- **Data type**: The pixel values are stored as 16-bit unsigned integers (uint16)\n",
    "- **Value range**: The pixel values range from 51 to 3884\n",
    "\n",
    "In optimap, videos are stored as 3D numpy arrays where the first dimension represents time (frames), and the second and third dimensions represent the height and width of each frame, respectively.\n",
    "\n",
    "### About the Example Video\n",
    "\n",
    "This example recording shows a ex-vivo isolated rabbit heart during ventricular fibrillation. THe heart was stained with a voltage-sensitive fluorescent dye (Di-4-ANEPPS) and recorded at 500 frames per second. The heart is contracting slightly, which causes motion in the video. The data is from {cite:t}`Chowdhary2023`.\n",
    "\n",
    "## 2. Playing Videos\n",
    "\n",
    "Let's look at our video data using optimap's built-in viewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Display the video with a title\n",
    "# skip_frame=3 means we only show every third frame (for speed)\n",
    "om.show_video(video, title='Heart during Fibrillation', skip_frame=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_video(video, title='Heart during Fibrillation', interval=25),\n",
    "       mp4_filename=\"VF_Rabbit_1_gui.mp4\",\n",
    "       save_args={\"hide_slider\": False, \"hide_buttons\": False, \"hide_framecounter\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video above, you should observe the heart tissue with subtle variations in brightness (fluorescence intensity) that represent electrical activity. These changes correspond to action potentials propagating across the cardiac tissue during ventricular fibrillation.\n",
    "\n",
    "You may also notice the slight motion of the heart tissue as it contracts. This movement creates what we call \"motion artifacts\" in optical mapping data, which can obscure the true electrical signals we're interested in.\n",
    "\n",
    "The complex wave patterns (action potential wavefronts) are somewhat difficult to see in this raw footage. In the video below, they were enhanced and are shown in purple. In the following sections, we'll demonstrate techniques to enhance these signals, compensate for the motion, and extract meaningful data from this optical mapping recording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "filepath = om.utils.download_example_data('VF_Rabbit_1_warped.npy', silent=True)\n",
    "video_warped = om.load_video(filepath, use_mmap=True)\n",
    "# video_warped_normalized = om.video.normalize_pixelwise_slidingwindow(video_warped, window_size=60)\n",
    "# signal = om.motion.reverse_motion_compensate(\n",
    "#     video, video_warped_normalized,\n",
    "#     contrast_kernel=5,\n",
    "#     presmooth_spatial=1,\n",
    "#     presmooth_temporal=1\n",
    "# )\n",
    "# om.save_video('VF_Rabbit_1_rwarped_normalized_f16.npy', r.astype(np.float16))\n",
    "filepath = om.utils.download_example_data('VF_Rabbit_1_rwarped_normalized_f16.npy', silent=True)\n",
    "signal = om.load_video(filepath)\n",
    "mask = om.background_mask(video_warped[0], show=False)\n",
    "signal[:, mask] = np.nan\n",
    "alpha = om.video.normalize(signal, vmin=0.5, vmax=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.video.show_video_overlay(video, overlay=(1-signal), alpha=alpha, vmax_overlay=1, interval=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Viewer: Monochrome\n",
    "\n",
    "[Monochrome](https://github.com/sitic/monochrome) is a companion software for visualizing monochromatic video data. In certain aspects, yt is more powerful and flexible than the built-in viewer functions in optimap ({func}`show_video`, {func}`show_video_overlay`, ...). You can use Monochrome to visualize the same video data we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# This will open the video in Monochrome in a new window\n",
    "import monochrome as mc\n",
    "mc.show(video, name=\"Heart Recording\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<center><img src=\"https://cardiacvision.github.io/optimap/main/_static/Monochrome-screenshot1.webp\"></center>](https://github.com/sitic/monochrome/)\n",
    "\n",
    "Monochrome allows you to click on the video to see the signal traces at specific locations, adjust brightness/contrast, and more.\n",
    "\n",
    "## 3. Extracting and Analyzing Signals\n",
    "\n",
    "In optical mapping, we're often interested in the changes in fluorescence over time at specific locations on the heart. These time-series are called \"traces\" and represent the electrical activity.\n",
    "\n",
    "### Selecting Traces Interactively\n",
    "\n",
    "Let's select some points on the heart and look at their signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# This opens an interactive window where you can click to select positions\n",
    "# fps=500 tells optimap the video was recorded at 500 frames per second\n",
    "traces, positions = om.select_traces(video, size=5, fps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "positions = [(127, 147), (130, 209), (202, 136)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11,4))\n",
    "om.trace.show_positions(positions, video[0], ax=axs[0])\n",
    "traces = om.trace.extract_traces(video, positions, size=5, ax=axs[1], show=True, fps=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interactive mode, you can:\n",
    "- **Left-click** on the image to add points\n",
    "- **Right-click** on a point to remove it\n",
    "- **Close** the window when you're done\n",
    "\n",
    "### Understanding Trace Extraction\n",
    "\n",
    "The `extract_traces` function pulls signals from specific locations in the video. Instead of using a single pixel (which can be noisy), optimap averages over a small window around each position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract traces from our previously selected positions\n",
    "# size=1 means we only use the exact pixel (no averaging)\n",
    "traces = om.extract_traces(video, positions, size=1, show=True, fps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the traces with `size=1` look more noisy compared to the previous traces with `size=5`.\n",
    "\n",
    "You can customize how the signals are extracted and displayed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the problem:** The traces show large fluctuations that make it hard to see the actual cardiac signals. These are **motion artifacts** caused by the heart tissue moving during recording. In the next section, we'll fix this problem!\n",
    "\n",
    "## 4. Compensating for Motion Artifacts\n",
    "\n",
    "Motion is a major challenge in optical mapping. When the heart contracts, the tissue moves under the camera, causing changes in the optical trace that aren't related to electrical activity. \n",
    "\n",
    "Optimap can track and correct for this motion using a process called \"motion compensation\". Tracking and stabilizing motion in fluorescence videos requires specialized algorithms, see [](motion_compensation) and {footcite:t}`Christoph2018a, Lebert2022` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Track and correct for motion in the video\n",
    "# This may take a few minutes depending on your computer\n",
    "video_warped = om.motion_compensate(video,\n",
    "                                    contrast_kernel=5,      # Size of contrast enhancement kernel\n",
    "                                    presmooth_spatial=1,    # Amount of spatial smoothing\n",
    "                                    presmooth_temporal=1)   # Amount of temporal smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# load pre-computed warped video to speed up docs build time\n",
    "filepath = om.utils.download_example_data('VF_Rabbit_1_warped.npy', silent=True)\n",
    "video_warped = om.load_video(filepath, use_mmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, the function works by:\n",
    "1. Tracking the motion of each pixel between frames\n",
    "2. Warping each frame to align with a reference frame\n",
    "3. Creating a new video where the tissue appears stationary\n",
    "\n",
    "Let's compare the original and motion-corrected videos side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Show original and motion-corrected videos side-by-side\n",
    "om.show_video_pair(video,\n",
    "                   video_warped,\n",
    "                   title1=\"Original (with motion)\",\n",
    "                   title2=\"Corrected (stabilized)\",\n",
    "                   skip_frame=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.video.play2(video, video_warped, title1=\"Original (with motion)\", title2=\"Corrected (stabilized)\", skip_frame=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences may be subtle, but in the right video (corrected), the heart tissue stays more stationary.\n",
    "\n",
    "### Examining Traces After Motion Correction\n",
    "\n",
    "Now let's see how our traces look with motion correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show traces from the same positions in the motion-corrected video\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11,4))\n",
    "om.trace.show_positions(positions, video[0], ax=axs[0])\n",
    "traces_corrected = om.trace.extract_traces(video_warped, positions, size=5, ax=axs[1], fps=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Big improvement!** Notice how the traces now show clearer patterns with much less noise. The baseline fluctuations have been significantly reduced, making it easier to see the actual electrical signals.\n",
    "\n",
    "## 5. Saving and Exporting Results\n",
    "\n",
    "Once you've processed your data, you might want to save it for later analysis or to share with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Save the motion-corrected video as a TIFF stack\n",
    "# This preserves all the data for future analysis\n",
    "om.video.save_video('motion_corrected_heart.tiff', video_warped)\n",
    "\n",
    "# Export as a video file that can be played in standard media players\n",
    "# fps=50 sets the playback speed\n",
    "om.video.export_video('motion_corrected_heart.mp4', video_warped, fps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Enhancing Cardiac Wave Visualization\n",
    "\n",
    "The electrical signals in optical mapping are often small compared to the background fluorescence. We can enhance them to make the cardiac waves more visible using normalization techniques. See [](signal_extraction) for more details.\n",
    "\n",
    "### Sliding-Window Normalization\n",
    "\n",
    "This technique adjusts each pixel's intensity based on its local minimum and maximum over time, making the waves stand out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply sliding-window normalization to enhance wave visualization\n",
    "# window_size=60 means we use 60 frames (120ms at 500fps) for each calculation\n",
    "video_warped_normalized = om.video.normalize_pixelwise_slidingwindow(video_warped, window_size=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Mask for the Background\n",
    "\n",
    "To focus only on the heart tissue, we'll create a mask that identifies the background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask that identifies background pixels\n",
    "mask = om.background_mask(video_warped[0])\n",
    "\n",
    "# Set all background pixels to NaN in our normalized video\n",
    "video_warped_normalized[:, mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](masks) provides on overview of several methods in optimap to create masks, from automatic detection to manual drawing.\n",
    "\n",
    "Now let's look at our pixelwise normalized video, where the action potential waves appear as dark waves moving across the heart tissue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.video.play(video_warped_normalized, interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.video.play(video_warped_normalized, interval=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Wave Visualization and Overlays\n",
    "\n",
    "In the video above, you can see the normalized cardiac electrical activity clearly. The normalization technique has helped to:\n",
    "\n",
    "1. **Amplify small changes** in fluorescence that represent electrical activity\n",
    "2. **Remove background variation** across the heart tissue\n",
    "3. **Highlight the wave dynamics** of electrical propagation during fibrillation\n",
    "\n",
    "The dark waves you observe represent action potentials (depolarization) moving across the tissue in complex patterns characteristic of fibrillation.\n",
    "\n",
    "### Creating Video Overlays\n",
    "\n",
    "We can further enhance visualization by creating **overlays** that combine the original video with the processed data. This allows us to see both the anatomical structure (from the original video) and the electrical activity (from the normalized video) simultaneously.\n",
    "\n",
    "In the next cells, we'll create an overlay where:\n",
    "- The background shows the original heart tissue\n",
    "- Colored overlay shows the electrical wave activity\n",
    "- Transparency (alpha) is controlled by the intensity of the normalized signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "alpha = om.video.normalize(video_warped_normalized, vmin=0.5, vmax=0)\n",
    "om.video.show_video_overlay(video_warped, overlay=(1-video_warped_normalized), alpha=alpha, interval=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "alpha = om.video.normalize(video_warped_normalized, vmin=0.5, vmax=0)\n",
    "render(lambda: om.video.show_video_overlay(video_warped, (1-video_warped_normalized), alpha=alpha, interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "- Try loading your own optical mapping data\n",
    "- Learn about activation maps and conduction velocity in later tutorials\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "test_name": "notebook1"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
