{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{currentmodule} optimap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from optimap.utils import jupyter_render_animation as render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{tip}\n",
    "Download this tutorial as a {download}`Jupyter notebook <converted/activation.ipynb>`, or a {download}`python script <converted/activation.py>` with code cells. We highly recommend using [Visual Studio Code](#vscode) to execute this tutorial.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Activation Maps\n",
    "\n",
    "This tutorial demonstrates how to compute local activation times and activation maps from cardiac optical mapping data using ``optimap``. Local activation times (often referred to as LATs) are times at which the tissue becomes electrically activated. \n",
    "Computing local activation times corresponds to determining when the optical signal in a given pixel passes a certain pre-defined threshold or intensity value. For instance, if the optical trace is normalized and fluctuates betwen [0,1] then the tissue could be defined as being 'electrically activated' when the time-series rises above or below 0.5 (depending on the fluorescent indicator and polarity of the signal).\n",
    "\n",
    "First, we load and preprocess an example dataset in which a planar action potential wave propagates across the ventricles of a rabbit heart during pacing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimap as om\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = om.download_example_data(\"Sinus_Rabbit_1.npy\")\n",
    "video = om.load_video(filename, frames=20)\n",
    "video = om.video.rotate_left(video)\n",
    "\n",
    "# motion compensation and normalization\n",
    "video_warped = om.motion.motion_compensate(video, 5, ref_frame=0)\n",
    "video_warped_norm = om.video.normalize_pixelwise(video_warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ``optimap``'s {func}`background_mask` function to blanck out the background in the image, such that the activation map is only computed for pixels showing tissue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove background by masking\n",
    "mask = om.background_mask(video_warped[0], show=False)\n",
    "mask = om.image.dilate_mask(mask, iterations=2)\n",
    "om.image.show_mask(mask, video_warped[0], title=\"Background mask\")\n",
    "video_warped_norm[:, mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the rabbit heart was stained with the voltage-sensitive dye Di-4-ANEPPS, the tissue becomes darker when it depolarizes (negative signal / polarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_video_pair(video, video_warped_norm, title1=\"original video\",\n",
    "                   title2=\"warped, normalized video\", interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_video_pair(video, video_warped_norm, title1=\"original video\", title2=\"warped, normalized video\", interval=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some of the video frames as the wave propagates across the ventricles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axs = plt.subplots(1, 6, figsize=(10, 3))\n",
    "axs[0].imshow(video[0], cmap='gray')\n",
    "axs[0].set_title('original')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "for i in range(1, 6):\n",
    "    axs[i].imshow(video_warped_norm[i*3], cmap='gray', vmin=0, vmax=1)\n",
    "    axs[i].set_axis_off()\n",
    "    time = (i*3) * (1000/500)  # 500 fps recording, show time in ms\n",
    "    axs[i].set_title(f\"{time:.0f} ms\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute an activation map by identifying the local activation times in each pixel that correspond to when the action potential wave front passes through that pixel.\n",
    "\n",
    "## Computing Activation Maps from Pixel-wise Normalized Optical Maps\n",
    "\n",
    "We will first compute an activation map with a pixel-wise normalized video. The pixel-wise normalized video contains values between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.print_properties(video_warped_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pixel-wise normalization was sufficient as opposed to a sliding-window pixel-wise normalization, see [Tutorial 2](signal_extraction.ipynb), because we isolated a short part of the video that is only 20 frames long. In other cases it might be necessary to use a sliding-window pixel-wise normalization or a frame-wise difference video (e.g. with motion), see below.\n",
    "\n",
    "Let's plot some of the optical traces (manually selected so that they show locations which become subsequently activated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions =  [(227, 181), (199, 162), (213, 171), (240, 189), (176, 146), (188, 153)]\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "om.trace.show_positions(positions, video[0], ax=axs[0])\n",
    "traces = om.extract_traces(video_warped_norm,\n",
    "                           positions,\n",
    "                           fps=500,\n",
    "                           ax=axs[1])\n",
    "axs[1].axhline(y=0.5, color='r', linestyle='dashed', label='threshold')\n",
    "axs[1].text(0.030, 0.52, 'threshold', color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ``optimap``'s {func}`compute_activation_map` function to automatically compute a two-dimensional activation map which shows the local activation times in every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = om.compute_activation_map(video_warped_norm, threshold=0.5,\n",
    "                                           inverted=True, fps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the argument `inverted=True` due to the negative polarity of the signal ($- \\Delta F / F$). If me had manually inverted the video beforehand or with calcium imaging data this would not be necessary. The range of local activation times can be displayed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.print_properties(activation_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the local activation times are given in milliseconds (based on argument `fps=500`) and they range between 0ms and 36ms. The function {func}`compute_activation_map` uses {func}`show_image` to plot the activation map (which can be disabled with argument `show=False`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.show_image(activation_map, cmap=\"jet\", title='Activation Map',\n",
    "              show_colorbar=True, colorbar_title='Activation Time [ms]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have plotted the activation map using the `jet` colormap, here are some other options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "om.show_image(activation_map, cmap='jet', show_colorbar=True, title='cmap=jet', ax=axs[0])\n",
    "om.show_image(activation_map, cmap='magma', show_colorbar=True, title='cmap=hsv', ax=axs[1])\n",
    "om.show_image(activation_map, cmap='twilight_shifted', show_colorbar=True, title='cmap=twilight_shifted', ax=axs[2])\n",
    "plt.suptitle('Activation maps with different colormaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Activation Maps from Frame-Wise Difference Optical Maps\n",
    "\n",
    "In [Tutorial 2](signal_extraction.ipynb), we introduced the frame-wise difference method to emphasize sudden temporal changes in a video. Sudden temporal changes are caused by upstrokes of the action potential or calcium transients and the frame-wise difference filter is therefore ideally suited to visualize wavefronts as they propagate across the tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_diff = om.video.temporal_difference(video_warped, 5)\n",
    "video_diff[:, mask] = np.nan\n",
    "video_diff_norm = om.video.normalize_pixelwise(video_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frame-wise difference approach enhances action potential upstroke, see the following video with temporal difference in the middle and our previous pixel-wise normalized video on the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_videos([video, video_diff_norm, video_warped_norm],\n",
    "               titles=[\"original\", \"warped, frame-wise diff\", \"warped, normalized\"],\n",
    "               interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_videos([video, video_diff_norm, video_warped_norm],\n",
    "               titles=[\"original\", \"warped, frame-wise diff\", \"warped, normalized\"],\n",
    "               interval=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the wavefront as an overlay over the raw (motion-stabilized) video. We will need to further post-process the data as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_diff[video_diff > 0] = 0\n",
    "video_diff_norm = om.video.normalize_pixelwise(-video_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action potential upstroke overlaid onto the raw video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.video.show_video_overlay(video_warped,\n",
    "                            overlay=video_diff_norm,\n",
    "                            vmin_overlay=-1,\n",
    "                            vmax_overlay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.video.show_video_overlay(video_warped, video_diff_norm, vmin_overlay=-1, vmax_overlay=1, interval=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute an activation map from the frame-wise difference video:\n",
    "\n",
    "```{warning}\n",
    "This tutorial is currently work in progress. We will add more information soon.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "test_name": "notebook1"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
