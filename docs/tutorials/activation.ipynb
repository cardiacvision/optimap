{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{currentmodule} optimap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from optimap.utils import jupyter_render_animation as render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{tip}\n",
    "Download this tutorial as a {download}`Jupyter notebook <converted/activation.ipynb>`, or a {download}`python script <converted/activation.py>` with code cells. We highly recommend using [Visual Studio Code](#vscode) to execute this tutorial.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Activation Maps\n",
    "\n",
    "This tutorial demonstrates how to compute and visualize activation maps from cardiac optical mapping data using ``optimap``. Activation maps display local activation times (LATs), which indicate when the tissue becomes electrically activated. These maps are crucial for understanding cardiac electrical activity and identifying abnormalities.\n",
    "\n",
    "Computing local activation times corresponds to determining when the optical signal in a given pixel passes a certain pre-defined threshold or intensity value. For instance, if the optical trace is normalized and fluctuates between [0, 1] then the tissue could be defined as being 'electrically activated' when the time-series rises above or below 0.5 (depending on the fluorescent indicator and polarity of the signal).\n",
    "\n",
    "Here, we will use an example data from {cite:t}`Rybashlykov2022` in which a planar action potential wave propagates across the ventricle of a mouse heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimap as om\n",
    "import monochrome as mc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = om.download_example_data(\"doi:10.5281/zenodo.5557829/mouse_41_120ms_control_iDS.mat\")\n",
    "video = om.load_video(filename)\n",
    "metadata = om.load_metadata(filename)\n",
    "print(f\"Loaded video with shape {video.shape} and metadata {metadata}\")\n",
    "frequency = metadata[\"frequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mouse_41_120ms_control_iDS.mat` file from the [Zenodo dataset](https://doi.org/10.5281/zenodo.5557829) shows a induced pacing beats in a mouse heart. The {func}`load_metadata` function loads the metadata from the MATLAB file, in this case the acquisition frame rate. We visualize the video using [Monochrome](https://github.com/sitic/Monochrome):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Show video\n",
    "mc.show(video, name=filename.name, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video('https://cardiacvision.ucsf.edu/sites/g/files/tkssra6821/f/optimap-mouse_41_120ms_control_iDS_monochrome.mp4', html_attributes='controls autoplay loop width=\"100%\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ``optimap``'s {func}`background_mask` function to create a mask of the heart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove background by masking\n",
    "mask = om.background_mask(video[0], show=False)\n",
    "mask = om.image.dilate_mask(mask, iterations=5, show=False)\n",
    "om.image.show_mask(mask, video[0], title=\"Background mask\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing activation maps is highly noise-sensitive, so we need to need to apply strong filtering to the data. Here we use a spatio-temporal Gaussian filter and a spatial mean filter to smooth the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_filtered = om.video.smooth_spatiotemporal(video, sigma_temporal=1, sigma_spatial=1)\n",
    "video_filtered = om.video.mean_filter(video_filtered, size_spatial=5)\n",
    "\n",
    "# Normalize the video using a pixelwise sliding window\n",
    "video_norm = om.video.normalize_pixelwise_slidingwindow(video_filtered, window_size=200)\n",
    "video_norm[:, mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used a pixelwise sliding window normalization to normalize the traces of each pixel to the range [0, 1]. We could have also used a regular pixel-wise normalization or a normalization off a frame-wise difference filtered video, see [Tutorial 2](signal_extraction.ipynb).\n",
    "\n",
    "Because the mouse heart was stained with the voltage-sensitive dye Di-4-ANEPPS, the tissue becomes darker when it depolarizes (negative signal / polarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_video_pair(video, video_norm, title1=\"original video\",\n",
    "                   title2=\"normalized video\", interval=10)\n",
    "\n",
    "# Or in Monochrome:\n",
    "#\n",
    "# mc.show(video, name=\"original video\")\n",
    "# mc.show(video_norm, name=\"normalized video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_video_pair(video, video_norm, title1=\"original video\", title2=\"normalized video\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the {func}`activation.find_activations` function to find pacing beats. Because the activation wave is so fast, we can just average the signal over the whole heart to identify when the pacing beat occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = om.activation.find_activations(1 - video_norm, fps=frequency)\n",
    "print(f\"Found {len(activations)} activation events at frames: {activations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function return a list of activation times in frames and plots detected activations with a red line. We plotted `1 - signal` here to show the normal action potential shape.\n",
    "\n",
    "To get a more accurate estimate when the first activation occurs per pacing beat we can manually select a pixel close to the pacing site and run it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a pixel position close to the pacing site to find activation events\n",
    "#\n",
    "# traces, coords = om.select_traces(video_norm[:500], size=10, ref_frame=video[0])\n",
    "# trace = om.extract_traces(video_norm, coords[0], size=10)\n",
    "# activations = om.activation.find_activations(1 - trace)\n",
    "\n",
    "# Here hardcoded position (141, 100) for demo purposes\n",
    "fig, axs = plt.subplot_mosaic('ABB', figsize=(10, 2))\n",
    "om.show_positions([(141, 100)], video[0], ax=axs['A'])\n",
    "trace = om.extract_traces(video_norm, (141, 100), size=10)\n",
    "pacing_events = om.activation.find_activations(1 - trace, ax=axs['B'], fps=frequency)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Found {len(pacing_events)} activation events at frames: {pacing_events}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the wave propagating across the ventricles for the first pacing beat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axs = plt.subplots(1, 7, figsize=(10, 3))\n",
    "om.show_image(video[0], ax=axs[0], title='original')\n",
    "for i in range(0, 6):\n",
    "    time = i*2 * (1000/frequency)  # convert frames to ms\n",
    "    om.show_image(video_norm[pacing_events[0] + i*2], title=f\"{time:.1f} ms\", ax=axs[i+1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Local Activation Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some of the optical traces (manually selected so that they show locations which become subsequently activated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positions selected with the GUI:\n",
    "# positions = om.select_positions(video[0])\n",
    "positions =  [(134, 101), (14, 93), (94, 99), (53, 97)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "om.trace.show_positions(positions, video[0], ax=axs[0])\n",
    "traces = om.extract_traces(video_norm,\n",
    "                           positions,\n",
    "                           size=10,\n",
    "                           fps=frequency,\n",
    "                           ax=axs[1])\n",
    "axs[1].axhline(y=0.5, color='r', linestyle='dashed', label='threshold')\n",
    "axs[1].text(0.03, 0.52, 'threshold', color='r')\n",
    "plt.xlim(0, 0.12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function {func}`find_activations` determines the time point at which the signal crosses the threshold (here 0.5). We will call this the local activation time (LAT). LATs can also be computed based on maximum derivative $\\frac{dV}{dt}$, but this is not implemented in ``optimap`` yet.\n",
    "\n",
    "```{note}\n",
    "The function{func}`find_activations` will return the **closest** frame index to the threshold crossing. This means that if the threshold is crossed in between two frames, the function will return the index of the frame that is closest to the threshold crossing.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "activations = om.activation.find_activations(traces, threshold=threshold, falling_edge=True, show=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "xlim = (130, 146)\n",
    "om.show_traces(traces, ax=ax, colors=colors, linestyle='solid', marker='.')\n",
    "for i in range(len(activations)):\n",
    "    for activation in activations[i]:\n",
    "        if 130 <= activation <= 146:\n",
    "            ax.axvline(activation, linestyle='--', color=colors[i], alpha=0.6)\n",
    "            ax.text(activation, 0.45, f'LAT: {activation:.1f}', \n",
    "                    rotation=90, va='top', ha='right', color=colors[i], fontsize=10)\n",
    "ax.axhline(y=threshold, color='r', linestyle='dashed')\n",
    "ax.text(143.5, threshold + 0.02, 'Threshold', color='r')\n",
    "ax.set_xlim(130, 146)\n",
    "ax.grid()\n",
    "fig.suptitle(\"Detected Local Activation Times (LATs)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Activation Maps\n",
    "\n",
    "We can now compute an activation map by identifying the local activation times in each pixel that correspond to when the action potential wave front passes through that pixel.\n",
    "\n",
    "``optimap``'s {func}`compute_activation_map` function automatically computes a two-dimensional activation map which shows the local activation times in every pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pacing_events[2]\n",
    "activation_map = om.compute_activation_map(\n",
    "    video_norm[t - 2:t + 16],\n",
    "    falling_edge=True,\n",
    "    fps=frequency,\n",
    "    vmax=15,\n",
    "    show_contours=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the argument `falling_edge=True` due to the negative polarity of the signal ($- \\Delta F / F$). If me had manually inverted the video beforehand or with calcium imaging data this would not be necessary.\n",
    "\n",
    "Because our mask was automatically generated, we have a lot of pixels which are not part of the ventricles. Let's improve the mask manually to create better activation maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine mask manually with the GUI:\n",
    "#\n",
    "# mask = om.interactive_mask(image=video[0], initial_mask=mask)\n",
    "# om.save_mask('mouse_41_120ms_control_iDS_mask.png', mask)\n",
    "\n",
    "# Loading the mask from the file for demo purposes\n",
    "mask_filename = om.download_example_data('mouse_41_120ms_control_iDS_mask.png')\n",
    "mask = om.load_mask(mask_filename)\n",
    "video_norm[:, mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it again. In the activation map below which show contour lines, which are a powerful visualization tool and help highlight the wavefront propagation. Contour lines are not shown by default, but can be added by setting `show_contours=True` in the {func}`compute_activation_map` or {func}`show_activation_map` functions. Here we also define custom contour levels of our choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = om.compute_activation_map(\n",
    "    video_norm[t - 3:t + 17],\n",
    "    falling_edge=True,\n",
    "    fps=frequency,\n",
    "    show_contours=True,\n",
    "    contour_levels=[3, 6, 9, 12],\n",
    "    vmax=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation map is a 2D array with the LAT value in **frames** for each pixel. Both {func}`compute_activation_map` and {func}`find_activations` return results in terms of frames, the plotting functions {func}`activation.show_activations` and {func}`show_activation_map` convert it to milliseconds using the `fps` frame rate parameter.\n",
    "\n",
    "If `normalize_time` is set to `True` (default) the minimum LAT is subtracted from all LAT values, so that the first activation time is at frame 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.print_properties(activation_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the latest activation (LAT) at frame 16.\n",
    "\n",
    "{func}`compute_activation_map` uses {func}`find_activation` to calculate the local activation time (LAT) for each pixel. By default the _closest_ time point to the threshold crossing is used, see left panel below.\n",
    "\n",
    "When `interpolation=True` is set in either function, the LAT is calculated as the fractional time between the two frames that cross the threshold using linear interpolation (right panel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax=axs[0]\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "xlim = (130, 146)\n",
    "for ax, interpolate in zip(axs, [False, True]):\n",
    "    om.show_traces(traces, ax=ax, colors=colors, linestyle='solid', marker='.')\n",
    "    activations = om.activation.find_activations(1 - traces, interpolate=interpolate, show=False)\n",
    "    for i in range(len(activations)):\n",
    "        for activation in activations[i]:\n",
    "            if xlim[0] <= activation <= xlim[1]:\n",
    "                ax.axvline(activation, linestyle='--', color=colors[i], alpha=0.6)\n",
    "                ax.text(activation, 0.45, f'LAT: {activation:.1f}', \n",
    "                        rotation=90, va='top', ha='right', color=colors[i], fontsize=10)\n",
    "    ax.axhline(y=0.5, color='r', linestyle='dashed')\n",
    "    ax.text(xlim[1] - 3, 0.51, 'threshold', color='r')\n",
    "    ax.set_xlim(*xlim)\n",
    "    ax.grid()\n",
    "    ax.set_title(f\"interpolate=True\" if interpolate else \"interpolate=False (default)\")\n",
    "fig.suptitle(\"Non-interpolated vs interpolated LATs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the comparison of the activation maps with and without LAT interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map_smooth = om.compute_activation_map(\n",
    "    video_norm[t-3:t+17],\n",
    "    falling_edge=True,\n",
    "    interpolate=True,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
    "om.show_activation_map(activation_map,\n",
    "                       ax=axs[0],\n",
    "                       show_colorbar=False,\n",
    "                       title='interpolate=False',\n",
    "                       vmax=15)\n",
    "om.show_activation_map(activation_map_smooth, \n",
    "                       ax=axs[1],\n",
    "                       show_colorbar=False,\n",
    "                       title='interpolate=True',\n",
    "                       vmax=15)\n",
    "om.show_activation_map(activation_map_smooth,\n",
    "                       ax=axs[2],\n",
    "                       show_contours=True,\n",
    "                       contour_levels=[3, 6, 9, 12, 15],\n",
    "                       contour_fmt=' %.1f ms ',\n",
    "                       vmax=15,\n",
    "                       title='with contours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See {func}`show_activation_map` for more options to customize the activation map visualization.\n",
    "\n",
    "Let's compare activation maps across subsequent pacing beats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 3, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "for i in range(0, 12):\n",
    "    t = pacing_events[i]\n",
    "    om.compute_activation_map(\n",
    "        video_norm[t - 4:t + 20],\n",
    "        title=f\"Beat {i}\",\n",
    "        falling_edge=True,\n",
    "        fps=frequency,\n",
    "        ax=axs[i],\n",
    "        show_colorbar=False,\n",
    "        vmax=16\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at beat 7 more closely, and overlay the contour lines on top of the raw image. The contour levels are set to 2 ms intervals, but you can adjust them as needed.\n",
    "\n",
    "```{note}\n",
    "You can also combine contour lines with a raw image to visualize the propagation path over the heart tissue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pacing_events[7]\n",
    "activation_map = om.compute_activation_map(video_norm[t - 4:t + 20], falling_edge=True, interpolate=True, show=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Show the raw camera image\n",
    "om.show_image(video[t], ax=ax)\n",
    "# Add a black boundary line around the mask\n",
    "mask_boundary = ax.contour(~mask, levels=[0], colors='black', linewidths=1.5, alpha=0.8)\n",
    "# Show contours\n",
    "om.show_activation_map(\n",
    "    activation_map,\n",
    "    ax=ax,\n",
    "    fps=frequency,\n",
    "    show_map=False,\n",
    "    show_contours=True,\n",
    "    contour_levels=range(2, 20, 2),\n",
    "    contour_fontsize=10,\n",
    "    contour_args={'linewidths': 1.5, 'alpha': 0.8, 'cmap': 'turbo', 'colors': None})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have plotted the activation map using the `turbo` colormap, but you can choose any colormap you prefer using the `cmap` argument. `vmin` and `vmax` can be used to set the minimum and maximum values for the color scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 4))\n",
    "om.show_activation_map(activation_map, cmap='jet', show_colorbar=True, title='cmap=jet', ax=axs[0], fps=frequency, colorbar_title=\"\", vmax=18)\n",
    "om.show_activation_map(activation_map, cmap='magma', show_colorbar=True, title='cmap=magma', ax=axs[1], fps=frequency, colorbar_title=\"\", vmax=18)\n",
    "om.show_activation_map(activation_map, cmap='twilight_shifted', show_colorbar=True, fps=frequency, title='cmap=twilight_shifted', ax=axs[2], colorbar_title=\"\", vmax=18)\n",
    "plt.suptitle('Activation maps with different colormaps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using {func}`matplotlib.pyplot.quiver` we can visualize the propagation direction of the wavefront. The `quiver` function creates a 2D field of arrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth activation map and calculate gradient of the activation map\n",
    "activation_map_smooth2 = om.image.smooth_gaussian(activation_map_smooth, sigma=2)\n",
    "dy, dx = np.gradient(activation_map_smooth2)\n",
    "\n",
    "# Filter out 0.01% of the largest gradient values\n",
    "gradient_magnitude = np.sqrt(dx**2 + dy**2)\n",
    "threshold = np.nanpercentile(gradient_magnitude, 99.9)\n",
    "dx[gradient_magnitude > threshold] = np.nan\n",
    "dy[gradient_magnitude > threshold] = np.nan\n",
    "\n",
    "step = 8  # adjust step size for arrow density\n",
    "shape = activation_map_smooth2.shape\n",
    "y_indices, x_indices = np.mgrid[:shape[0], :shape[1]]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "ax = axs[0]\n",
    "\n",
    "om.show_activation_map(activation_map_smooth2, ax=ax, fps=frequency, vmax=15)\n",
    "ax.quiver(x_indices[::step, ::step],\n",
    "          y_indices[::step, ::step],\n",
    "          dx[::step, ::step],\n",
    "          dy[::step, ::step],\n",
    "          color='black',\n",
    "          pivot='mid',\n",
    "          angles='xy',\n",
    "          scale=4)\n",
    "# Add a black boundary line around the mask\n",
    "mask_boundary = ax.contour(~mask, levels=[0], colors='black', linewidths=1.5, alpha=0.8)\n",
    "\n",
    "ax = axs[1]\n",
    "step = 12\n",
    "om.show_image(video[0], ax=ax)\n",
    "ax.quiver(x_indices[::step, ::step],\n",
    "          y_indices[::step, ::step],\n",
    "          dx[::step, ::step],\n",
    "          dy[::step, ::step],\n",
    "          activation_map_smooth2[::step, ::step] / 15,\n",
    "          width=0.005,\n",
    "          cmap='turbo',\n",
    "          pivot='mid',\n",
    "          angles='xy',\n",
    "          scale=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly {func}`matplotlib.pyplot.streamplot` can also be used to visualize the propagation direction of the wavefront. The `streamplot` function creates a 2D field of streamlines, which are lines that follow the flow of the vector field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the activation map and calculate the gradient\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "ax = axs[0]\n",
    "om.show_activation_map(activation_map_smooth2, ax=ax, fps=frequency, vmax=15)\n",
    "ax.streamplot(x_indices, y_indices, dx, dy, color='white', linewidth=1)\n",
    "\n",
    "ax = axs[1]\n",
    "om.show_image(video[0], ax=ax)\n",
    "ax.streamplot(x_indices, y_indices, dx, dy, color=activation_map_smooth2, cmap='turbo', linewidth=1, arrowsize=1.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"Action Potential Wave Propagation Direction using streamplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Activation Maps from Frame-Wise Difference Optical Maps\n",
    "\n",
    "```{warning}\n",
    "This rest of this tutorial is currently work in progress. We will add more information soon.\n",
    "```\n",
    "\n",
    "In [Tutorial 2](signal_extraction.ipynb), we introduced the frame-wise difference method to emphasize sudden temporal changes in a video. Sudden temporal changes are caused by upstrokes of the action potential or calcium transients and the frame-wise difference filter is therefore ideally suited to visualize wavefronts as they propagate across the tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_diff = om.video.temporal_difference(video_filtered, 5)\n",
    "video_diff[:, mask] = np.nan\n",
    "video_diff_norm = om.video.normalize_pixelwise(video_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frame-wise difference approach enhances action potential upstroke, see the following video with temporal difference in the middle and our previous pixel-wise normalized video on the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_videos([video, video_diff_norm, video_norm],\n",
    "               titles=[\"original\", \"frame-wise diff\", \"pixelwise normalized\"],\n",
    "               interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_videos([video, video_diff_norm, video_norm],\n",
    "               titles=[\"original\", \"frame-wise diff\", \"pixelwise normalized\"],\n",
    "               interval=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the wavefront as an overlay over the raw (motion-stabilized) video. We will need to further post-process the data as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_diff[video_diff > 0] = 0\n",
    "video_diff_norm = om.video.normalize_pixelwise(-video_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action potential upstroke overlaid onto the raw video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.video.show_video_overlay(video,\n",
    "                            overlay=video_diff_norm,\n",
    "                            vmin_overlay=-1,\n",
    "                            vmax_overlay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.video.show_video_overlay(video, video_diff_norm, vmin_overlay=-1, vmax_overlay=1, interval=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "test_name": "notebook1"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
