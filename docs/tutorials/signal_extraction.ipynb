{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{currentmodule} optimap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Code snippet for rendering animations in the docs\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def render_ani_func(f):\n",
    "    om.utils.disable_interactive_backend_switching()\n",
    "    plt.switch_backend('Agg')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        ani = f()\n",
    "    %matplotlib inline\n",
    "    om.utils.enable_interactive_backend_switching()\n",
    "\n",
    "    vid = HTML(ani.to_html5_video())\n",
    "    plt.close('all')\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{tip}\n",
    "Download this tutorial as a {download}`Jupyter notebook <converted/signal_extraction.ipynb>`, or a {download}`python script <converted/signal_extraction.py>` with code cells.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Measurement of Fluorescent Signals and Waves\n",
    "\n",
    "This tutorial will cover the measurement of fluorescent signals and wave dynamics from cardiac optical mapping data more generally. In [Tutorial 1](basics.ipynb) we already loaded and displayed video data, performed numerical motion tracking and motion-stabilization, performed some post-processing, and displayed action potential waves. In this tutorial, we will cover the post-processing routines used to extract and display the optical signals in more detail. We first repeat the steps from the previous tutorial by loading the experimental data and performing motion correction. Note that the motion correction step is not required if you analyze data that was acquired with Blebbistatin or other pharmacological uncoupling or motion-inhibition techniques (you then simply skip this step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import optimap as om\n",
    "import numpy as np\n",
    "\n",
    "filepath = om.utils.retrieve_example_data('Example_02_VF_Rabbit_Di-4-ANEPPS_Basler_acA720-520um.npy')\n",
    "# alterantively set filepath = '/folder_on_your_computer/Example_02_VF_Rabbit_Di-4-ANEPPS_Basler_acA720-520um.npy'\n",
    "video = om.load_video(filepath)\n",
    "video_warped = om.motion_compensate(video,\n",
    "                              contrast_kernel=5,\n",
    "                              presmooth_spatial=1,\n",
    "                              presmooth_temporal=1)\n",
    "om.video.play(video_warped, title=\"recording without motion\", skip_frame=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Hidden, same as above but shorten the video slightly\n",
    "\n",
    "import optimap as om\n",
    "import numpy as np\n",
    "\n",
    "filepath = om.utils.retrieve_example_data('Example_02_VF_Rabbit_Di-4-ANEPPS_Basler_acA720-520um.npy')\n",
    "video = om.load_video(filepath, use_mmap=True)[:500]\n",
    "filepath = om.utils.retrieve_example_data('Example_02_VF_Rabbit_Di-4-ANEPPS_Basler_acA720-520um_warped.npy', silent=True)\n",
    "video_warped = om.load_video(filepath, use_mmap=True)[:500]\n",
    "render_ani_func(lambda: om.video.play(video_warped, title=\"recording without motion\", skip_frame=1, interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring action potentials or calcium transients in cardiac optical mapping videos is conventionally performed pixel-by-pixel. In the absence of motion, either with Blebbistatin or after numerical motion inhibition, each pixel shows the same tissue segment throughout the video. This condition is a prerequisite for the following processing routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Fluorescenct Signals and Waves from raw Video Data\n",
    "\n",
    "Voltage- or calcium-sensitive dyes or indicators modulate the intensity of the tissue in response to changes in the transmembrane potential or intracellular calcium concentration, respectively. When we extract and plot time-series from a video which show the pixel's intensities over time we will see small fluctuations or optical signals which correspond to action potentials or calcium transients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [(192, 167), (204, 141), (118, 158), (183, 267)]\n",
    "traces = om.extract_traces(video_warped, positions, size=3, show=True, fps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the optical signals are relatively small compared to the overall background fluorescence ($\\Delta F$ vs. $F$). Each time-series has its own particular baseline, which correlates with the brightness in the video image. In this example, the background is in the order of 2000-4000 intensity counts, and the small signal fluctuations are in the order of 100-200 intensity counts (the data was generated with a Basler acA720-520um camera with a 12-bit dynamic range). This signal needs to be isolated from the raw data. In other terms, the background fluorescence needs to be removed and the remaining signal needs to be amplified or \"renormalized\" before it can be further processed or visualized. This background-removal and amplification process can be done using 3 different approaches:\n",
    "\n",
    "1. Using pixel-wise normalization: This method conserves the shape of the action potential (under ideal conditions) and normalizes the signal to [0, 1].\n",
    "2. Using sliding-window pixel-wise normalization: This method is a variant of method 1 which also conserves the shape of the action potential (under ideal conditions and with restrictions) and normalizes the signal to [0, 1] using a sliding window. However, it can be sensitive to noise and changes of the base fluorescence due to inhomogeneous illumination when using motion correction.\n",
    "3. Computing the frame (or temporal) difference: This method is more robust than methods 1 and 2, however, it does not conserve the shape of the action potential. It is useful to visualize the propagation of the front of the action potential waves.\n",
    "\n",
    "### Pixel-wise Normalization\n",
    "\n",
    "Pixel-wise normalization refers to renormalizing each time-series or optical trace obtained from a single pixel individually. Renormalizing means that first the minimum of the time-series is subtracted from all values in the time-series, which removes the baseline or background fluorescence, and then all values are divided by the range (maximum value - minimum value) of the time-series, which produces a signal that fluctuates between 0 and 1. In ideal conditions, inactivated tissue then corresponds to 0 and the peaks of action potentials or calcium transients correspond to 1. The function {func}`video.normalize_pixelwise` performs this operation automatically by computing the following equation for each pixel $(x, y)$ and frame $t$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{signal}_{\\text{norm}}(t, x, y) = \\frac{\\text{signal}(t, x, y) - \\text{min}_{t} \\text{signal}(t, x, y)}{\\text{max}_{t} \\text{signal}(t, x, y) - \\text{min}_{t} \\text{signal}(t, x, y)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_warped_pnorm = om.video.normalize_pixelwise(video_warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel-wise normalized video (pnorm) looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.video.play(video_warped_pnorm, title=\"sliding window normalization\", interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render_ani_func(lambda: om.video.play(video_warped_pnorm, title=\"sliding window normalization\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optical traces sampled from the pixel-wise normalized video (pnorm) in the same locations as above look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_pnorm = om.extract_traces(video_warped_pnorm, positions, size=3, show=True, fps=500)\n",
    "traces_pnorm = om.extract_traces(video_warped_pnorm, positions[0], size=3, show=True, fps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline was removed and all signals now lie within the interval [0,1]. However, you will notice that pixel-wise normalization does not remove baseline drifts or modulations. Moreover, we did not take into account noise, which can skew the normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding-Window Pixel-wise Normalization\n",
    "\n",
    "The sliding/rolling window normalization performs a pixel-based normalization of the signal to [0, 1] within a short temporal window for each time point. The function {func}`video.normalize_pixelwise_slidingwindow` performs this operation, by computing the following equation for each pixel $(x, y)$ and frame $t$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{signal}_{\\text{norm}}(t, x, y) = \\frac{\\text{signal}(t, x, y) - \\text{min}_{t' \\in [t - w/2, t + w/2]} \\text{signal}(t', x, y)}{\\text{max}_{t' \\in [t - w/2, t + w/2]} \\text{signal}(t', x, y) - \\text{min}_{t' \\in [t - w/2, t + w/2]} \\text{signal}(t', x, y)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $w$ is the window size.\n",
    "\n",
    "Let's use a window size $w$ of 60 frames (120 ms) for this example and mask out the background.\n",
    "\n",
    "```{warning}\n",
    "This tutorial is currently work in progress. We will add more information soon.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_norm = om.video.normalize_pixelwise_slidingwindow(video_warped, window_size=60)\n",
    "\n",
    "om.video.play(video_warped_norm, title=\"sliding window normalization\", interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_norm = om.video.normalize_pixelwise_slidingwindow(video_warped, window_size=60)\n",
    "\n",
    "render_ani_func(lambda: om.video.play(video_warped_norm, title=\"sliding window normalization\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to blank-out everything else other than tissue, we can use ``optimap``'s mask function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_mask = om.background_mask(video_warped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_norm = om.video.normalize_pixelwise_slidingwindow(video_warped, window_size=60)\n",
    "video_warped_norm[:, background_mask] = np.nan\n",
    "\n",
    "om.video.play(video_warped_norm, title=\"sliding window normalization\", interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_norm = om.video.normalize_pixelwise_slidingwindow(video_warped, window_size=60)\n",
    "video_warped_norm[:, background_mask] = np.nan\n",
    "\n",
    "render_ani_func(lambda: om.video.play(video_warped_norm, title=\"sliding window normalization\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better visualization in the next steps we'll need a mask of the background. Here we use {func}`background_mask` to create a mask of the background from the first frame of the data using an automatic threshold.\n",
    "\n",
    "Pixels in red are considered background (value of `True`), and pixels in blue are considered foreground (value of `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "alpha = om.video.normalize(video_warped_norm, vmin=0.5, vmax=0)\n",
    "om.video.play_with_overlay(video_warped, 1-video_warped_norm, alpha=alpha, vmin_overlay=0, vmax_overlay=1, interval=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "alpha = om.video.normalize(video_warped_norm, vmin=0.5, vmax=0)\n",
    "render_ani_func(lambda: om.video.play_with_overlay(video_warped, (1-video_warped_norm), alpha=alpha, interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference\n",
    "\n",
    "The temporal difference approach computes the difference between frames at time $t$ and $t - n$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{signal}_{\\text{diff}}(t, x, y) = \\text{signal}(t, x, y) - \\text{signal}(t - n, x, y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\Delta t$ is the time difference between frames. This method is more robust, however it does generally not conserve the shape of the action potential. Rather, it isolates the front of the fluorescent waves.\n",
    "\n",
    "Let's use $n = 10$ for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "diff = om.video.temporal_difference(video_warped, n)\n",
    "diff[:, background_mask] = np.nan\n",
    "abs_max = 0.33*np.nanmax(np.abs(diff))\n",
    "om.video.play(diff, title=\"temporal difference\", cmap=\"PiYG\", vmin=-abs_max, vmax=abs_max, interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "diff[diff > 0] = 0\n",
    "om.video.play(diff, title=\"temporal difference\", cmap=\"PiYG\", vmin=-abs_max, vmax=abs_max, interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.video.play2(video_warped_norm, diff, title1=\"Sliding Window\", title2=\"Frame Difference\", vmin1=0, vmax1=1, cmap2='gray', vmin2=0.2*np.nanmin(diff), vmax2=0, interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "diff_norm = om.video.normalize_pixelwise_slidingwindow(diff, window_size=60)\n",
    "om.video.play2(video_warped_norm, diff_norm, vmin1=0, vmax1=1, cmap2='gray', vmin2=0, vmax2=1, interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "diff = om.video.temporal_difference(video_warped, 10)\n",
    "diff[diff > 0] = 0\n",
    "diff[:, background_mask] = np.nan\n",
    "diff_norm = om.video.normalize_pixelwise_slidingwindow(-diff, window_size=60)\n",
    "om.video.play_with_overlay(video_warped_norm, diff_norm, vmin_overlay=-1, vmax_overlay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "diff = om.video.temporal_difference(video_warped, 10)\n",
    "diff[diff > 0] = 0\n",
    "diff[:, background_mask] = np.nan\n",
    "diff_norm = om.video.normalize_pixelwise_slidingwindow(-diff, window_size=60)\n",
    "render_ani_func(lambda: om.video.play_with_overlay(video_warped, diff_norm, vmin_overlay=-1, vmax_overlay=1, interval=20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "test_name": "notebook1"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
