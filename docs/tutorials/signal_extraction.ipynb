{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{currentmodule} optimap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from optimap.utils import jupyter_render_animation as render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{tip}\n",
    "Download this tutorial as a {download}`Jupyter notebook <converted/signal_extraction.ipynb>`, or a {download}`python script <converted/signal_extraction.py>` with code cells.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Measurement of Fluorescent Signals and Waves\n",
    "\n",
    "This tutorial will cover the measurement of fluorescent signals and wave dynamics from cardiac optical mapping data more generally. In [Tutorial 1](basics.ipynb) we already loaded and displayed video data, performed numerical motion tracking and motion-stabilization, performed some post-processing, and displayed action potential waves. In this tutorial, we will cover the post-processing routines used to extract and display the optical signals in more detail. We first repeat the steps from the previous tutorial by loading the experimental data and performing motion correction. Note that the motion correction step is not required if you analyze data that was acquired with Blebbistatin or other pharmacological uncoupling or motion-inhibition techniques (you then simply skip this step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import optimap as om\n",
    "import numpy as np\n",
    "\n",
    "filepath = om.download_example_data('VF_Rabbit_1.npy')\n",
    "video = om.load_video(filepath)\n",
    "video_warped = om.motion_compensate(video,\n",
    "                              contrast_kernel=5,\n",
    "                              presmooth_spatial=1,\n",
    "                              presmooth_temporal=1)\n",
    "om.show_video(video_warped, title=\"recording without motion\", skip_frame=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Hidden, same as above but shorten the video slightly\n",
    "\n",
    "import optimap as om\n",
    "import numpy as np\n",
    "\n",
    "filepath = om.download_example_data('VF_Rabbit_1.npy')\n",
    "video = om.load_video(filepath, use_mmap=True, frames=500)\n",
    "filepath = om.download_example_data('VF_Rabbit_1_warped.npy', silent=True)\n",
    "video_warped = om.load_video(filepath, use_mmap=True, frames=500)\n",
    "render(lambda: om.show_video(video_warped, title=\"recording without motion\", skip_frame=1, interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring action potentials or calcium transients in cardiac optical mapping videos is conventionally performed pixel-by-pixel. In the absence of motion, either with Blebbistatin or after numerical motion inhibition, each pixel shows the same tissue segment throughout the video. This condition is a prerequisite for the following processing routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Fluorescenct Signals and Waves from raw Video Data\n",
    "\n",
    "Voltage- or calcium-sensitive dyes or indicators modulate the intensity of the tissue in response to changes in the transmembrane potential or intracellular calcium concentration, respectively. When we extract and plot time-series from a video which show the pixel's intensities over time we will see small fluctuations or optical signals which correspond to action potentials or calcium transients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [(192, 167), (204, 141), (118, 158), (183, 267)]\n",
    "traces = om.extract_traces(video_warped, positions, size=3, show=True, fps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the optical signals are small compared to the overall background fluorescence ($\\Delta F$ vs. $F$). Moreover, each time-series has its own particular baseline, which correlates with the brightness in the video image. In this example, the background is in the order of 2000-4000 intensity counts, and the small signal fluctuations are in the order of 100-200 intensity counts (the data was generated with a Basler acA720-520um camera with a 12-bit dynamic range). This signal needs to be isolated from the raw data. In other terms, the background fluorescence needs to be removed and the remaining signal needs to be amplified or \"renormalized\" before it can be further processed or visualized. This background-removal and amplification process can be done using 3 different approaches:\n",
    "\n",
    "1. Using pixel-wise normalization: This method conserves the shape of the action potential (under ideal conditions) and normalizes the signal to [0, 1].\n",
    "2. Using sliding-window pixel-wise normalization: This method is a variant of method 1 which also conserves the shape of the action potential (under ideal conditions and with restrictions) and normalizes the signal to [0, 1] using a sliding window. However, it can be sensitive to noise and changes of the base fluorescence due to inhomogeneous illumination when using motion correction.\n",
    "3. Computing the frame (or temporal) difference: This method is more robust than methods 1 and 2, however, it does not conserve the shape of the action potential as it amplifies rapid temporal changes in the signal. It is useful to visualize the propagation of the front of the action potential waves.\n",
    "\n",
    "### 1. Pixel-wise Normalization\n",
    "\n",
    "Pixel-wise normalization refers to renormalizing each time-series or optical trace obtained from a single pixel individually. Renormalizing means that first the minimum of the time-series is subtracted from all values in the time-series, which removes the baseline or background fluorescence, and then all values are divided by the range (maximum value - minimum value) of the time-series, which produces a signal that fluctuates between 0 and 1. In ideal conditions, inactivated tissue then corresponds to 0 and the peaks of action potentials or calcium transients correspond to 1. A pixel-wise normalized signal can be computed for each pixel $(x, y)$ and frame $t$ using the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{signal}_{\\text{norm}}(t, x, y) = \\frac{\\text{signal}(t, x, y) - \\text{min}_{t} \\text{signal}(t, x, y)}{\\text{max}_{t} \\text{signal}(t, x, y) - \\text{min}_{t} \\text{signal}(t, x, y)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The function {func}`video.normalize_pixelwise` performs this operation automatically and produces a pixel-wise normalized video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_warped_pnorm = om.video.normalize_pixelwise(video_warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel-wise normalized video looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_video(video_warped_pnorm, title=\"pixel-wise normalization\", interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_video(video_warped_pnorm, title=\"pixel-wise normalization\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel-wise normalization amplifies temporal fluctuations and therefore emphasizes action potential waves (or equally calcium waves). In our case, the action potential waves correspond to dark/black waves due to the particular staining. The normalization presrves the shape of the action potential (vs. method 3 detailed further below). The shape of the negative excursion corresponds to the shape of the action potential (in our case inverted due to the staining and emission filter). Regions without signal become noise (e.g. the background). You may have noticed that the video gets darker towards the end. The pixel-wise normalization does not account for overall brightness changes or baseline drifts as the maximal and minimal values are only determined once from the entire time-series. Correspondingly, the optical traces sampled from the pixel-wise normalized video look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_pnorm = om.extract_traces(video_warped_pnorm, positions, size=3, show=True, fps=500)\n",
    "traces_pnorm = om.extract_traces(video_warped_pnorm, positions[0], size=3, show=True, fps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different baselines were removed and all signals now lie within the interval [0,1]. However, as mentioned above, baseline drifts remain because they cannot be removed using the pixel-wise normalization (however, the sliding-window version below can). Moreover, due to the noise, the normalization does not scale the signal all the way onto the interval [0,1] because the minima and maxima are outliers resulting from the noise. In order to obtain better results, the video data would have to be smoothed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sliding-Window Pixel-wise Normalization\n",
    "\n",
    "The sliding-window (or rolling-window) pixel-wise normalization performs a pixel-by-pixel normalization of the signal to [0, 1] within a short temporal window for each time point. For each point in time, a new minimum and maximum is calculated within a temporal window around the point. The function {func}`video.normalize_pixelwise_slidingwindow` performs this operation by computing the following equation for each pixel $(x, y)$ and frame $t$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{signal}_{\\text{norm}}(t, x, y) = \\frac{\\text{signal}(t, x, y) - \\text{min}_{t' \\in [t - w/2, t + w/2]} \\text{signal}(t', x, y)}{\\text{max}_{t' \\in [t - w/2, t + w/2]} \\text{signal}(t', x, y) - \\text{min}_{t' \\in [t - w/2, t + w/2]} \\text{signal}(t', x, y)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $w$ is the window size. The window size $w$ needs to be adjusted to the period of the action potential or calcium wave activity and should not be shorter than the period and ideally be roughly 1-3 periods long. In contrast to the pixel-wise normalization, the sliding-window pixel-wise normalization can inhibit baseline drifts. If the window is too long, baseline drifts will not be inhibited as effectively as with shorter windows. We will use a window size $w$ of 60 frames (corresponds to 120 ms) for this example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_warped_spnorm = om.video.normalize_pixelwise_slidingwindow(video_warped, window_size=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sliding-window pixel-wise normalized video looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_video(video_warped_spnorm, title=\"sliding window normalization\", interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render(lambda: om.show_video(video_warped_spnorm, title=\"sliding window normalization\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences between the sliding-window pixel-wise normalized and the pixel-wise normalized videos are subtle but you may notice that unlike the simply pixel-wise normalized video this video does not get darker towards the end. Accordingly, the optical traces sampled from the sliding-window pixel-wise normalized video show much less of a baseline drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_spnorm = om.extract_traces(video_warped_spnorm, positions, size=3, show=True, fps=500)\n",
    "traces_spnorm = om.extract_traces(video_warped_spnorm, positions[0], size=3, show=True, fps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sliding-window pixel-wise normalization reduces baseline drift and enhances the contrast between activated and repolarized tissue in the video. The residual baseline modulations occur because the sliding-window normalization was performed on the noisy raw video data. The results will improve with smoothed video data, see [Tutorial 10](smoothing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Frame-wise / Temporal Difference\n",
    "\n",
    "The frame-wise or temporal difference approach emphasizes temporal intensity changes in the videos and computes the difference between frames at time $t$ and $t - \\Delta t$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{signal}_{\\text{diff}}(t, x, y) = \\text{signal}(t, x, y) - \\text{signal}(t - \\Delta t, x, y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\Delta t$ is the time difference between frames (usually set to $\\Delta t = 1-5$ frames). This method computes the temporal derivative of the optical traces and therefore amplifies temporal changes in the image (which can occur both due to fluorescent signals or motion). Without motion, the frame-wise difference method amplifies and highlights sudden changes in intensity which coincide with the upstroke of the action potential or calcium transient (and to some extent also with the repolariztion phase). The shorter $\\Delta t$ the more emphasizes is put on sudden changes. With motion and longer $\\Delta t$, it also amplifies optical flow. The frame difference method is more robust than the (sliding-window) pixel-wise normalization method. However it does generally not conserve the shape of the action potential. Rather, it isolates the front of the fluorescent waves.\n",
    "\n",
    "For simplicity, let's use $n$ instead of $\\Delta t$ and set $n = 5$ for this example.\n",
    "\n",
    "```{warning}\n",
    "This tutorial is currently work in progress. We will add more information soon.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "diff = om.video.temporal_difference(video_warped, n)\n",
    "#diff[:, background_mask] = np.nan\n",
    "abs_max = 0.33*np.nanmax(np.abs(diff))\n",
    "om.show_video(diff, title=\"temporal difference\", cmap=\"PiYG\", vmin=-abs_max, vmax=abs_max, interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "diff[diff > 0] = 0\n",
    "om.show_video(diff, title=\"temporal difference\", cmap=\"PiYG\", vmin=-abs_max, vmax=abs_max, interval=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the sliding-window pixel-wise normalization and frame difference methods next to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.show_video_pair(video_warped_spnorm, diff, title1=\"Sliding Window\", title2=\"Frame Difference\", vmin1=0, vmax1=1, cmap2='gray', vmin2=0.2*np.nanmin(diff), vmax2=0, interval=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can and should also renormalize the frame difference video and compare the sliding-window pixel-wise normalization and sliding-window pixel-wise normalized frame difference methods next to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "diff_norm = om.video.normalize_pixelwise_slidingwindow(diff, window_size=60)\n",
    "om.show_video_pair(video_warped_spnorm, diff_norm, vmin1=0, vmax1=1, cmap2='gray', vmin2=0, vmax2=1, interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "diff = om.video.temporal_difference(video_warped, 10)\n",
    "diff[diff > 0] = 0\n",
    "#diff[:, background_mask] = np.nan\n",
    "diff_norm = om.video.normalize_pixelwise_slidingwindow(-diff, window_size=60)\n",
    "om.video.show_video_overlay(video_warped, diff_norm, vmin_overlay=-1, vmax_overlay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "diff = om.video.temporal_difference(video_warped, 10)\n",
    "diff[diff > 0] = 0\n",
    "#diff[:, background_mask] = np.nan\n",
    "diff_norm = om.video.normalize_pixelwise_slidingwindow(-diff, window_size=60)\n",
    "render(lambda: om.video.show_video_overlay(video_warped, diff_norm, vmin_overlay=-1, vmax_overlay=1, interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Post-Processing\n",
    "\n",
    "We can use ``optimap``'s mask function to blank-out the black background (which appears as noise): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_mask = om.background_mask(video_warped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "video_warped_spnorm[:, background_mask] = np.nan\n",
    "\n",
    "om.show_video(video_warped_spnorm, title=\"sliding window normalization\", interval=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_spnorm = om.video.normalize_pixelwise_slidingwindow(video_warped, window_size=60)\n",
    "video_warped_spnorm[:, background_mask] = np.nan\n",
    "\n",
    "render(lambda: om.show_video(video_warped_spnorm, title=\"sliding window normalization\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better visualization in the next steps we'll need a mask of the background. Here we use {func}`background_mask` to create a mask of the background from the first frame of the data using an automatic threshold.\n",
    "\n",
    "Pixels in red are considered background (value of `True`), and pixels in blue are considered foreground (value of `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "alpha = om.video.normalize(video_warped_spnorm, vmin=0.5, vmax=0)\n",
    "om.video.show_video_overlay(video_warped, 1-video_warped_spnorm, alpha=alpha, vmin_overlay=0, vmax_overlay=1, interval=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "alpha = om.video.normalize(video_warped_spnorm, vmin=0.5, vmax=0)\n",
    "render(lambda: om.video.show_video_overlay(video_warped, (1-video_warped_spnorm), alpha=alpha, interval=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "test_name": "notebook1"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
