{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "```{currentmodule} optimap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Code snippet for rendering animations in the docs\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def render_ani_func(f):\n",
    "    om.utils.disable_interactive_backend_switching()\n",
    "    plt.switch_backend('Agg')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        ani = f()\n",
    "    %matplotlib inline\n",
    "    om.utils.enable_interactive_backend_switching()\n",
    "\n",
    "    vid = HTML(ani.to_html5_video())\n",
    "    plt.close('all')\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Download this tutorial as a {download}`Jupyter notebook <converted/ratiometry.ipynb>`, or a {download}`python script <converted/ratiometry.py>` with code cells.\n",
    "```\n",
    "\n",
    "# Tutorial 7: Ratiometry\n",
    "\n",
    "## Combining Numerical Motion Compensation and Ratiometry with Di-4-ANEPPS\n",
    "\n",
    "In this tutorial, we will discuss the analysis of ratiometric optical mapping data and the benefits of a combined approach consisting of numerical motion tracking and ratiometric imaging. Numerical motion tracking and motion-stabilization alone cannot inhibit motion artifacts entirely, see Tutorials [1](basics.ipynb) & [4](motion_compensation.ipynb). Numerical motion tracking alone cannot remove all motion artifacts alone, because it does not remove the relative motion between the tissue and the light sources used to excite the fluorescent dye. Motion tracking is merely a change of the frame of reference. The physical relative motion persists afer numerical motion-stabilization and causes fluctuations in the illumination. Typically, what can be seen in motion-stabilized videos is that the illumination (e.g. a Gaussian intensity distribution) moves back and forth across the tissue which leads to illumination artifacts. Motion artifacts therefore often have two components: dissociation-related motion artifacts and illumination artifacts. To achieve better artifact compensation, particularly with stronger motion, it is therefore required to combine numerical motion tracking with ratiometric imaging, as described in {cite:t}`Kappadan2020` or in {cite:t}`Zhang2016`. \n",
    "\n",
    "In the example below, the heart was stained with Di-4-ANEPPS and imaged using alternating green and blue illumination at 500fps. More specifically, the tissue was illuminated in every odd frame with green light and in every even frame with blue light, respectively. This approach is also referred to as 'excitation ratiometry'. After numerical motion tracking and stabilization, the green and blue videos are divided by each other to obtain a motion-stabilized ratiometric video. Using this method, both motion artifacts as well as illumination artifacts (which also lead to motion artifacts) can be significantly reduced. \n",
    "\n",
    "First, we load the video data, which is stored as a single file (`Example_05_Ratiometry.npy`) containing the two green and blue videos in an interleaved fashion (frame 1 = green, frame 2 = blue, frame 3 = green, etc.). Using optimap's {func}`load_video()` function, we can specify to load only every 2nd frame and to start reading from the first or the second frame, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimap as om\n",
    "\n",
    "filename = om.utils.retrieve_example_data('Example_05_Ratiometry.npy')\n",
    "video_blue = om.load_video(filename, start_frame=0, step=2)\n",
    "video_green = om.load_video(filename, start_frame=1, step=2)\n",
    "\n",
    "om.print_properties(video_blue)\n",
    "om.print_properties(video_green)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one could load the video and manually split the video into the green and blue videos (`video_green = video[::2,:,:]` etc.). Both videos contain 'uint16' intensity values in each pixel, which means that they can have whole unsigned integer values between 0 and 65535. The videos dimensions are 128 x 128 pixels and each video contains 600 frames. The original interleaved video contains 1200 frames. The green video is only slightly brighter based on the maximum value.\n",
    "\n",
    "When we play the two videos next to each other we notice that there is signal in only the green video but not in the blue video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.video.play2(video_green, video_blue,\n",
    "               title1=\"green video with signal\",\n",
    "               title2=\"blue video without signal\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "render_ani_func(lambda: om.video.play2(video_green, video_blue, title1=\"green video with signal\", title2=\"blue video without signal\", interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot and compare two optical traces from the green and blue videos respectively, we can see not in all but in most areas stronger intensity fluctuations in the green video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.trace.set_default_trace_window('disc') # the trace is sample from a disc-shaped region with a diameter specified by 'size'\n",
    "om.compare_traces([video_green, video_blue],\n",
    "                  labels=['green','blue'],\n",
    "                  colors=['green','blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the motion in both videos is not compensated yet and produces strong motion artifacts. In the green video, the intensity fluctuations are caused in parts by motion and in parts by the optical signal related to the action potential. In the blue video, nearly all fluctuations are caused by motion. Let's look at a few specific examples. Above, the function {func}`compare_traces()` was started in interactive mode because we did not provide specific coordinates as arguments. We can also provide specific coordinates, e.g. the pixel at (50,50) or a list of pixels, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [(77, 57), (72, 74), (65, 42), (80, 75), (90, 90)]\n",
    "om.compare_traces([video_green, video_blue],\n",
    "                  positions,\n",
    "                  labels=['green','blue'],\n",
    "                  colors=['green', 'blue'],\n",
    "                  fps=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the some of these examples, the green optical trace shows steep downstrokes which is typical for the upstroke of the action potential, whereas the blue trace does not exhibit such downstrokes and exhibits much weaker and inconsistent intensity fluctuations. These fluctuations are largely caused by motion, whereas the green trace has a signal component that is caused by the fluorescent dye. You can best explore this behavior using the interactive mode. We can visualize from where these traces were sampled as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.show_positions(video_green[0], positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform numerical motion tracking and motion-stabilization with the green and blue videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_green = om.motion_compensate(video_green, contrast_kernel=5, ref_frame=0)\n",
    "video_warped_blue = om.motion_compensate(video_blue, contrast_kernel=5, ref_frame=0)\n",
    "om.video.playn([video_green, video_warped_green, video_blue, video_warped_blue],\n",
    "               titles=[\"video green\", \"stabilized video green\", \"video blue\", \"stabilized video blue\"],\n",
    "               figsize=(8, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_green = om.motion_compensate(video_green, contrast_kernel=5, ref_frame=0)\n",
    "video_warped_blue = om.motion_compensate(video_blue, contrast_kernel=5, ref_frame=0)\n",
    "render_ani_func(lambda: om.video.playn([video_green, video_warped_green, video_blue, video_warped_blue],\n",
    "               titles=[\"video green\", \"stabilized video green\", \"video blue\", \"stabilized video blue\"],\n",
    "               figsize=(8, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the comparison above, you can see that both the green and blue videos were succesfully tracked and warped (motion-stabilized). There is no residual motion after warping and there are no tracking artifacts. Please refer to {cite:t}`Christoph2018a,Kappadan2020,Lebert2022` for details. Internally, the function {func}`motion_compensate()` computed a contrast-enhanced video, registered / tracked and stabilized the motion / warped the original video. Now let's look at how the green and blue traces have changed after the motion-stabilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.compare_traces([video_green, video_warped_green],\n",
    "                  positions,\n",
    "                  labels=['green','warped green'],\n",
    "                  size=3,\n",
    "                  colors=['lightgreen', 'green'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted the motion-stabilized green traces slightly darker than the original green trace. In this example, the motion-stabilized traces are more consistent, exhibit less variability and less deflections in the repolarization phase of the action potential. In one example, the action potential shape was not visible at all before motion-stabilization, but becomes visible after motion-stabilization. The blue traces change as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.compare_traces([video_blue, video_warped_blue],\n",
    "                  positions,\n",
    "                  labels=['blue','warped blue'],\n",
    "                  size=3,\n",
    "                  colors=['lightblue', 'blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted the motion-stabilized blue traces in dark blue and the original traces in light/grey blue. Here the changes are not obvious or systematic, because we are largely looking at motion artifacts. If anything then the traces become more regular. You can explore these changes yourself using the interactive mode of the {func}`compare_traces()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "om.compare_traces([video_green, video_warped_green],\n",
    "                  labels=['green','warped green'],\n",
    "                  size=3,\n",
    "                  colors=['lightgreen', 'green'],\n",
    "                  fps=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine the green and blue motion-stabilized videos to obtain a motion-stabilized ratiometric video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_ratio = video_warped_green/video_warped_blue\n",
    "om.video.playn([video_green, video_warped_green, video_warped_blue, video_warped_ratio],\n",
    "               titles=[\"video green\", \"stabilized video green\", \"stabilized video blue\", \"stabilized video ratio\"],\n",
    "               figsize=(8, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "video_warped_ratio = video_warped_green/video_warped_blue\n",
    "render_ani_func(lambda: om.video.playn([video_green, video_warped_green, video_warped_blue, video_warped_ratio],\n",
    "               titles=[\"video green\", \"stabilized video green\", \"stabilized video blue\", \"stabilized video ratio\"], \n",
    "               figsize=(8, 3),\n",
    "               interval=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to compare the motion-stabilized green traces with the corresponding motion-stabilized ratiometric traces, we need to renormalize the videos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_warped_green_norm = om.video.normalize_pixelwise_slidingwindow(video_warped_green, window_size=100)\n",
    "video_warped_ratio_norm = om.video.normalize_pixelwise_slidingwindow(video_warped_ratio, window_size=100)\n",
    "\n",
    "#norm_raw = om.video.normalize_pixelwise_slidingwindow(video, window_size=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we compare the motion-stabilized green traces with the corresponding motion-stabilized ratiometric traces we can see slight improvements of the signal quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.compare_traces([video_warped_green_norm, video_warped_ratio_norm],\n",
    "                  positions,\n",
    "                  labels=['warped green','ratio'],\n",
    "                  size=3,\n",
    "                  colors=['g', 'k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These subtle improvements become more evident when you explore the data yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.compare_traces([video_warped_green_norm, video_warped_ratio_norm],\n",
    "                  labels=['green','ratio'],\n",
    "                  size=3,\n",
    "                  colors=['green', 'k'],\n",
    "                  fps=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and zoom in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 150\n",
    "om.compare_traces([video_warped_green_norm[:t], video_warped_ratio_norm[:t]],\n",
    "                  labels=['green','ratio'],\n",
    "                  colors=['green', 'k'],\n",
    "                  size=3,\n",
    "                  fps=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action potential duration (APD) is more heterogeneous in non-ratiometric motion-stabilized videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_blue, positions = om.select_traces(video_blue)\n",
    "traces_green = om.extract_traces(video_green, positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: use flow field from blue channle to compensate the green channel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
